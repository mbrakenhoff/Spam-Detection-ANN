---
title: "Detecting Spam - ANN with Parameter Tuning"
author: "Michael Brakenhoff"
output: 
  html_document:
    toc: yes
    toc_float: yes
---

```{r setup, include=FALSE}
rm(list=ls())
library(data.table)
library(MLmetrics)
library(keras)
library(fastDummies)
library(tidyverse)
library(knitr)
library(pROC)
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(warning = F)
knitr::opts_chunk$set(message = F)
knitr::opts_chunk$set(fig.align = "center")

load('final_workspace.RData') #Workspace load after tuning of parameters
```

# Purpose

To predict if email is spam, an artificial neural network is built and tuned on a data set of 4601 observations of 57 independent variables.

# Artificial Neural Network Functions

```{r functions}
#  INCLUDED FOR QUICK REFERENCE. FULL FILE AND TUNING FOUND IN Model_Tune.R
#  nn_mod = function(dat, response, params) {
#   dat = dummy_cols(dat, remove_selected_columns = T, remove_first_dummy=T)
#   response = paste0(response, '_spam')
#   
#   X = as.matrix(dat[,-which(colnames(dat)==response)])
#   y = to_categorical(dat[,response])
#   
#   mod = keras_model_sequential()
#   
#   mod %>% 
#     layer_dense(units = 200, input_shape = ncol(X)) %>% 
#     layer_activation(activation = params$ACT1) %>% 
#     layer_dense(units = 150, kernel_regularizer = regularizer_l1(.01)) %>% 
#     layer_activation(activation = params$ACT2) %>% 
#     layer_dense(units = 2, kernel_regularizer = regularizer_l1(.01)) %>%
#     layer_activation(activation = 'sigmoid')
#  
#   # sgd = optimizer_sgd(lr = .01, momentum = .6, decay = 1e-5)
#    
#   mod %>% 
#     compile(loss = 'binary_crossentropy',
#             optimizer = params$OPT,
#             metrics = c('accuracy'))
#   
#   class_weight = list('0' = 6., '1' = 4.)
#   
#   
#   mod %>% fit(X, y, epochs = 200, batch_size = params$BATCH, validation_split = 0.15, class_weight = class_weight,
#               callbacks = list(callback_early_stopping(monitor='val_loss', min_delta = 0.001, patience = params$PATIENCE, 
#                                                        mode = 'min')))
#   
#   return(mod)
# }
# 
# nn_outer_perf = function(mod, dat, response) {
#   dat = dummy_cols(dat, remove_selected_columns = T, remove_first_dummy=T)
#   response = paste0(response, '_spam')
#   
#   X = as.matrix(dat[,-which(colnames(dat)==response)])
#   y = dat[,response]
#   
#   preds = predict(mod, X)[,2]
#   preds = ifelse(preds > .5, 1, 0)
#   
#   return(Accuracy(preds, y))
# }
# 
# nn_inner_perf = function(mod, dat, response) {
#   dat = dummy_cols(dat, remove_selected_columns = T, remove_first_dummy=T)
#   response = paste0(response, '_spam')
#   
#   X = as.matrix(dat[,-which(colnames(dat)==response)])
#   y = dat[,response]
#   
#   preds = predict(mod, X)[,2]
#   preds = ifelse(preds > .5, 1, 0)
#   
#   return(Accuracy(preds, y))
# }
# 
# nn_score = function(mod, dat, response) {
#   dat = dummy_cols(dat, remove_selected_columns = T, remove_first_dummy=T)
#   response = paste0(response, '_spam')
#   
#   X = as.matrix(dat[,-which(colnames(dat)==response)])
#   y = dat[,response]
#   
#   probs = predict(mod, X)[,2]
#   
#   return(probs)
# }

```


# Parameter Tuning 
```{r Tuning1}
# INCLUDED FOR QUICK REFERENCE. FULL FILE AND TUNING FOUND IN Model_Tune.R
#  nn_grid = data.frame(list(ACT1 = c('relu', 'tanh'), ACT2 = c('relu', 'tanh'), OPT = c('adam', 'sgd'), BATCH = 30, PATIENCE = c(10,30))
#                      %>% cross_df())
# 
# nn_sel = nested_cv(cv_k1 = 4, cv_k2 = 5, seed = 1, model = nn_mod, outer_perf_f = nn_outer_perf, inner_perf_f = nn_inner_perf,
#                    perf_type = 'high', grid = nn_grid, score_f = nn_score, dat = spam, response = 'type')
perf = nn_sel$Performance
best_sel = nn_sel$Performance$BestParams[which.max(nn_sel$Performance$Perf)]
top4 = nn_grid[c(2,9,10,12),]
```

A 4-Fold cross-validation (5-fold inner) was utilized to determine the best layer activation, model optimizer, batch size (previous runs), and patience for early stopping on validation loss. After running multiple grid searches, the following 4 options of parameters where chosen:  
`r kable(top4)`  
Choosing the best performing set at each fold provided the following performance:  
`r kable(perf)`  
The mean performance being, `r round(mean(perf$Perf),2)` .
It was observed that the the following parameters performed the best consistently. Only the parameter for early stopping, patience, varied.  
`r kable(nn_grid[10, -length(nn_grid)])`  
Therefore, these parameters were chosen to perform another grid search to optimize the patience parameter.  

# Parameter Tuning 2
```{r tuning 2}
# INCLUDED FOR QUICK REFERENCE. FULL FILE AND TUNING FOUND IN Model_Tune.R
#  nn_grid2 = data.frame(list(ACT1 = 'tanh', ACT2 = 'relu', OPT = 'adam', BATCH = 30, PATIENCE = c(10, 20, 30, 40, 50))
#                       %>% cross_df())
# 
# nn_sel2 = nested_cv(cv_k1 = 4, cv_k2 = 5, seed = 1, model = nn_mod, outer_perf_f = nn_outer_perf, inner_perf_f = nn_inner_perf,
#                    perf_type = 'high', grid = nn_grid2, score_f = nn_score, dat = spam, response = 'type')
perf2 = nn_sel2$Performance
best_sel2 = nn_sel2$Performance$BestParams[which.max(nn_sel2$Performance$Perf)]
Patience = nn_grid2[nn_sel2$Performance$BestParams,"PATIENCE"]
top = cbind(Performance = perf2$Perf, Patience)
#row 1
```

An expanded search of patience provided varying results:  
`r kable(top)`  
The mean performance, `r round(mean(perf2$Perf),4)` .
Since no setting appears to consistently improve the performance, a middle setting of 30 will be used for patience. This should average out the underfitting and overfitting in the folds.  

# Final Performance
```{r final model}
# INCLUDED FOR QUICK REFERENCE. FULL FILE AND TUNING FOUND IN Model_Tune.R
#  nn_params = nn_grid2[3,]
# nn_final = nested_cv(cv_k1 = 5, cv_k2 = 2, seed = 1, model = nn_mod, outer_perf_f = nn_outer_perf, inner_perf_f = nn_inner_perf,
#                     perf_type = 'high', grid = nn_params, score_f = nn_score, dat = spam, response = 'type')
final_perf = nn_final$Performance
final_mean = round(mean(nn_final$Performance$Perf),4)
```

The final model is fit across a 5-fold cross-validation, giving the final performance estimate of:  
`r kable(final_perf[,"Perf"], col.names = 'Performance', row.names = c(1:5))`  
Mean Performance: `r final_mean`.  
The plot of performance across the 5 folds shows consistent accuracy around 94% and the ROC curve indicates it is a very good model.


```{r CV_ROC Plot}
X = data.frame(list(Performance = nn_final$Performance$Perf,
                    Fold = c(rep(1:5, 1))
                    ))
X %>%
  ggplot(aes(Fold, Performance))+
  geom_point()+
  geom_line()+
  theme_bw()+
  ylab("Performance (Accuracy)")+
  ggtitle("Model Performance Across 5-Fold CV")

roc_final = roc(nn_final$Y, nn_final$Scores)
ggroc(roc_final) + theme_minimal() + labs(title = paste0('ROC Curve - AUC: ', round(pROC::auc(roc_final),3)))
```
